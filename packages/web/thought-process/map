# Intelligent Video Compilation Platform - Technical Flow Map

## üéØ System Overview
Transform passive video archives into an active, searchable, creative canvas through AI analysis and natural language processing.

**Core Philosophy**: Upload ‚Üí AI Analysis ‚Üí Natural Language Search ‚Üí Automated Video Compilation

---

## üèóÔ∏è Google Cloud Platform Architecture

**Next.js Full-Stack App on Cloud Run**
           ‚Üï
**Firebase Auth** ‚Üî **Firestore** ‚Üî **Cloud Storage** ‚Üî **AI Pipeline**

### Complete Tech Stack
- **Framework**: Next.js 14 with TypeScript
- **Frontend**: React components with Tailwind CSS
- **Backend**: Next.js API routes (serverless functions)
- **Hosting**: Google Cloud Run (auto-scaling containers)
- **Authentication**: Firebase Auth (Google OAuth + email/password)
- **Database**: Firestore (real-time, user data, upload sessions)
- **Storage**: Cloud Storage (video files)
- **AI Services**: Video Intelligence API + Speech-to-Text API
- **State Management**: React Context API
- **Package Manager**: npm

---

## üìã Phase 1: Authentication & User Management

### 1.1 Landing Page Flow
1. **Homepage**: Minimal, clean interface explaining the platform value
   - Hero section: "Upload videos, search with AI, create compilations"
   - Two main CTAs: "Sign Up" and "Sign In" buttons
   - Simple, professional design with Tailwind CSS
2. **No Guest Access**: All features require authentication for cross-device persistence

### 1.2 Authentication System
**Sign Up Flow**:
1. User clicks "Sign Up" ‚Üí Sign up form page
2. **Two Options**:
   - Google OAuth (one-click, recommended)
   - Email/password form (email, password, confirm password)
3. **Account Creation**: Create user document in Firestore
4. **Redirect**: Successful signup ‚Üí Upload page (Phase 2)

**Sign In Flow**:
1. User clicks "Sign In" ‚Üí Sign in form page  
2. **Two Options**:
   - Google OAuth
   - Email/password form
3. **Authentication**: Firebase Auth validation
4. **Redirect**: Successful login ‚Üí Upload page (Phase 2)

**Forgot Password Flow**:
1. "Forgot Password?" link on sign in page
2. Email input form
3. Firebase sends password reset email
4. User resets password via email link
5. Redirect back to sign in page

### 1.3 Database Schema (Firestore)

**Users Collection** (`users/{userId}`):
```json
{
  "uid": "firebase_user_id",
  "email": "user@example.com", 
  "authProvider": "google" | "email",
  "createdAt": "timestamp",
  "lastLoginAt": "timestamp",
}
```

### 1.4 UI Components Structure
**Pages**:
- `/` - Landing page with auth CTAs
- `/auth/signup` - Sign up form  
- `/auth/signin` - Sign in form
- `/auth/forgot-password` - Password reset form
- `/upload` - Main upload interface (Phase 2)

**Components**:
- `AuthLayout` - Shared layout for auth pages
- `SignUpForm` - Email/password + Google OAuth signup
- `SignInForm` - Email/password + Google OAuth signin  
- `ForgotPasswordForm` - Password reset request
- `AuthProvider` - Context for auth state management

### 1.5 Navigation Flow
```
Homepage (/) 
    ‚îú‚îÄ‚îÄ Sign Up ‚Üí /auth/signup ‚Üí Account Creation ‚Üí /upload
    ‚îú‚îÄ‚îÄ Sign In ‚Üí /auth/signin ‚Üí Authentication ‚Üí /upload
    ‚îî‚îÄ‚îÄ Sign In ‚Üí Forgot Password ‚Üí /auth/forgot-password ‚Üí Email Sent ‚Üí /auth/signin
```

### 1.6 Authentication State Management
- **Firebase Auth**: Handles all authentication logic
- **React Context**: `AuthProvider` wraps entire app
- **Route Protection**: Upload page requires authentication
- **Persistent Sessions**: Users stay logged in across browser sessions
- **Auto-redirect**: Authenticated users accessing auth pages redirect to /upload

### Key Benefits of This Architecture
- **Cross-device continuity**: Users can switch devices seamlessly
- **Secure by default**: All video data tied to authenticated users
- **Scalable auth**: Firebase handles user management automatically
- **Simple UX**: Minimal friction to get users into the main upload flow
- **Future-ready**: Database structure supports Phase 2 video AI features

---

## üìã Phase 2: Upload & Initial Processing

### 1.1 Upload Initiation Flow
1. **User Authentication Check**: Verify user is signed in
2. **File Selection**: Drag & drop or file picker interface
3. **File Validation**: Check type, size, and user quota limits
4. **Adaptive Chunking**: Determine optimal chunk size based on file size
   - Small files (< 1GB): 5MB chunks
   - Medium files (1-10GB): 10MB chunks  
   - Large files (> 10GB): 20MB chunks
5. **Upload Session Creation**: Generate unique video ID and save to Firestore

### 1.2 Secure Upload URL Generation
1. **Cloud Function Trigger**: Frontend requests upload URLs
2. **User Verification**: Validate JWT token and user permissions
3. **Signed URL Creation**: Generate time-limited upload URLs for each chunk
4. **Session Storage**: Save upload session details to Firestore
5. **URL Response**: Return array of signed URLs to frontend

### 1.3 Direct Upload Process
1. **Parallel Chunk Upload**: Upload multiple chunks simultaneously to Cloud Storage
2. **Real-time Progress**: Update UI and Firestore after each successful chunk
3. **Error Handling**: Retry failed chunks with exponential backoff
4. **State Persistence**: All progress saved to Firestore (not localStorage)
5. **Resume Capability**: Users can continue from any device/browser

### 1.4 Upload Completion & Reassembly
1. **Completion Verification**: Check all chunks uploaded successfully
2. **Cloud Function Trigger**: Frontend calls finalize endpoint
3. **Chunk Verification**: Verify all chunks exist in Cloud Storage
4. **Video Reassembly**: Use Cloud Storage compose operation to merge chunks
5. **Status Update**: Mark video as ready for processing in Firestore
6. **Pipeline Trigger**: Initiate AI analysis phase

### 1.5 Video Library Interface Flow
1. **Library Access**: Floating toggle button at bottom of upload page
2. **Library Overlay**: Slides up as modal overlay (doesn't navigate away)
3. **Card Generation**: Query Firestore for user's videos, render as cards
4. **Real-time Updates**: Subscribe to Firestore changes for live status updates
5. **Card Display**: Each card shows video name + visual status indicator

### 1.6 Video Library Status System
**Status Indicators**:
- **Uploading**: Progress ring with percentage + upload arrow icon
- **Paused/Failed**: Red warning icon with retry option
- **Processing**: Spinning gear icon (video reassembly/AI analysis)
- **Completed**: Green checkmark with play button

**Card Information**:
- Video display name (user can rename inline)
- Upload date/time
- File size
- Current status with appropriate icon
- Progress bar (if uploading/processing)

### 1.7 Library Data Flow
1. **Firestore Query**: Get all videos where `userId == currentUser.uid`
2. **Status Mapping**: Map database status to UI indicator
3. **Real-time Sync**: Firestore onSnapshot listener updates cards instantly
4. **User Actions**: Rename video ‚Üí Update Firestore ‚Üí UI reflects change
5. **Upload Resume**: Click paused upload ‚Üí Resume chunked upload process

### 1.8 Library UI Behavior
- **Responsive Grid**: Cards arranged in responsive grid layout
- **Inline Editing**: Click video name to rename (saves to Firestore)
- **Status Actions**: Click status indicator for relevant actions (retry, resume, etc.)
- **Auto-refresh**: Library updates automatically as uploads progress
- **Close Library**: Click outside overlay or toggle button to close

### Key Features
- **Cross-device resume**: Upload state saved to user's cloud account
- **Intelligent retry**: Failed chunks automatically retried
- **Cost optimization**: Direct upload to storage (bypasses function data limits)
- **Security**: Time-limited signed URLs, user authentication required
- **Scalability**: Handles 5-minute to 12-hour videos efficiently
- **Real-time library**: Video cards update live during upload/processing

---

## üìã Phase 3: AI Analysis Pipeline - Complete Step-by-Step Flow

### 3.1 TRIGGER: Analysis Initiation
**Step 1**: Phase 2 upload finalization completes successfully
**Step 2**: `finalize/route.ts` updates video status in Firestore: `status: "ready_for_analysis"`
**Step 3**: Firestore trigger or Cloud Task queues analysis job with video metadata
**Step 4**: Cloud Run analysis container spins up and receives job parameters
**Step 5**: Update video status to `status: "analyzing"` ‚Üí User sees "Analyzing..." in video library

### 3.2 PREPARATION: Video Processing Setup
**Step 6**: Download original video file from Cloud Storage to analysis container
**Step 7**: Validate video file integrity and format compatibility
**Step 8**: Extract basic metadata: duration, resolution, frame rate, audio channels
**Step 9**: Convert video to standardized format if needed (1080p, H.264, AAC audio)
**Step 10**: Extract separate audio track (.wav format) for speech processing
**Step 11**: Calculate total expected 5-second segments: `Math.ceil(videoDuration / 5)`
**Step 12**: Create processing workspace directory structure in container

### 3.3 SEGMENTATION: Create 5-Second Clips
**Step 13**: Start segmentation loop from 0:00 to end of video
**Step 14**: For each 5-second window (0-5s, 5-10s, 10-15s, etc.):
  - Extract video segment using FFmpeg: `ffmpeg -ss START -t 5 -i input.mp4 segment_N.mp4`
  - Extract corresponding audio segment: `ffmpeg -ss START -t 5 -i input.wav segment_N.wav`
  - Generate segment thumbnail (frame at 2.5-second mark)
  - Create segment metadata object with start/end times
**Step 15**: Handle final segment (may be <5 seconds if video doesn't divide evenly)
**Step 16**: Upload all 5-second video segments to Cloud Storage: `videos/{userId}/{videoId}/segments/`
**Step 17**: Create segments array in memory for processing queue

### 3.4 AUDIO ANALYSIS: Speech Processing Per Segment
**Step 18**: Initialize Google Speech-to-Text API client with configuration:
  - Language: Auto-detect or user-specified
  - Speaker diarization: Enabled (max 6 speakers)
  - Word-level timestamps: Enabled
  - Audio events detection: Enabled

**Step 19**: Process each 5-second audio segment sequentially:
  - Upload audio segment to Speech-to-Text API
  - Receive transcription with speaker labels and timestamps
  - Extract non-speech audio events (laughter, music, applause, etc.)
  - Store results in segment analysis object

**Step 20**: Cross-segment speaker linking:
  - Compare speaker voice characteristics across segments
  - Build consistent speaker ID mapping (Speaker_1, Speaker_2, etc.)
  - Update all segment transcriptions with consistent speaker IDs

### 3.5 VISUAL ANALYSIS: Video Intelligence Per Segment
**Step 21**: Initialize Google Video Intelligence API client with features:
  - Object detection and tracking
  - Person detection and face recognition
  - Activity recognition
  - Scene change detection
  - Text detection (signs, captions)

**Step 22**: Process each 5-second video segment in parallel batches (5 segments at once):
  - Upload segment to Video Intelligence API
  - Request: Object detection, person detection, activity recognition
  - Receive: Objects with confidence scores, person bounding boxes, activities
  - Extract spatial relationships between detected elements
  - Store visual analysis results for each segment

**Step 23**: Cross-segment person linking:
  - Compare face embeddings across segments to identify same people
  - Build consistent person ID mapping (Person_1, Person_2, etc.)
  - Track person appearance/disappearance patterns
  - Update all segments with consistent person IDs

### 3.6 CROSS-MODAL LINKING: Connect Audio + Visual
**Step 24**: For each 5-second segment, perform audio-visual synchronization:
  - Identify when a person's mouth is moving (visual) + speech is detected (audio)
  - Link Speaker IDs to Person IDs when they occur simultaneously
  - Build speaker-to-face mapping table
  - Handle cases where multiple people speak or faces aren't visible

**Step 25**: Resolve conflicts and ambiguities:
  - If Speaker_1 maps to multiple faces, use confidence scores and context
  - If Person_1 sometimes speaks and sometimes doesn't, track speaking patterns
  - Create final Person profiles with both visual and audio characteristics

### 3.7 SCENE DESCRIPTION: Generate Rich Metadata
**Step 26**: For each 5-second segment, generate comprehensive description:

**People Analysis**:
- Count visible people and their descriptions
- Identify who is speaking vs. who is visible but silent
- Describe clothing, posture, facial expressions
- Note actions: "talking", "laughing", "walking", "gesturing"

**Object Detection**:
- List all medium/major objects with confidence >0.7
- Group related objects: "kitchen items", "furniture", "technology"
- Note object interactions: "person holding phone", "dog near couch"

**Activity Recognition**:
- Identify primary activity: "conversation", "cooking", "playing", "working"
- Note secondary activities: "background music playing", "dog walking by"
- Describe movement patterns: "person walking left to right", "camera panning"

**Spatial Context**:
- Determine scene type: "kitchen", "living room", "outdoor", "office"
- Note lighting conditions: "bright daylight", "dim evening", "artificial lighting"
- Describe camera perspective: "close-up", "wide shot", "over-shoulder view"

### 3.8 SEARCHABLE TEXT GENERATION
**Step 27**: For each segment, create optimized search strings:
- **People**: "blonde woman red shirt talking man blue jeans listening"
- **Objects**: "kitchen counter apples coffee mug laptop smartphone"
- **Actions**: "cooking chopping vegetables phone conversation laughing"
- **Context**: "kitchen daytime conversation casual cooking preparation"
- **Audio**: "upbeat talking chopping sounds phone notification"

**Step 28**: Combine all elements into master searchable text:
```
"blonde woman red shirt talking chopping vegetables kitchen counter apples knife cooking man blue jeans listening phone conversation smartphone notification casual daytime"
```

### 3.9 DATA STORAGE: Save to Firestore
**Step 29**: Create Firestore document structure for each segment:

```json
videos/{videoId}/segments/segment_{N}: {
  "segmentNumber": 1,
  "startTime": 0.0,
  "endTime": 5.0,
  "duration": 5.0,
  "videoPath": "videos/user123/vid789/segments/segment_001.mp4",
  "thumbnailPath": "videos/user123/vid789/thumbnails/segment_001.jpg",
  
  "people": [
    {
      "personId": "person_1",
      "description": "blonde woman in red sweater",
      "boundingBox": {"x": 0.2, "y": 0.1, "width": 0.3, "height": 0.8},
      "actions": ["talking", "chopping"],
      "speakerLabel": "Speaker_1",
      "confidence": 0.94,
      "isSpeaking": true,
      "facialExpression": "focused"
    }
  ],
  
  "objects": [
    {"name": "kitchen counter", "confidence": 0.98, "boundingBox": {...}},
    {"name": "knife", "confidence": 0.91, "boundingBox": {...}},
    {"name": "carrots", "confidence": 0.87, "boundingBox": {...}}
  ],
  
  "activities": [
    {"name": "cooking", "confidence": 0.93},
    {"name": "food preparation", "confidence": 0.89},
    {"name": "conversation", "confidence": 0.85}
  ],
  
  "audio": {
    "transcript": "So I think we should add more seasoning to this",
    "speakerCount": 1,
    "audioEvents": ["chopping sounds", "kitchen ambiance"],
    "volume": "moderate",
    "clarity": "clear"
  },
  
  "sceneContext": {
    "location": "kitchen",
    "timeOfDay": "daytime", 
    "lighting": "natural bright",
    "cameraAngle": "medium shot",
    "mood": "casual"
  },
  
  "searchableText": "blonde woman red sweater talking chopping kitchen counter knife carrots cooking food preparation conversation seasoning daytime natural lighting",
  
  "processingMetadata": {
    "analyzedAt": "2024-01-15T10:33:15Z",
    "videoIntelligenceTime": 12.3,
    "speechToTextTime": 8.7,
    "totalProcessingTime": 21.0,
    "apiCosts": {
      "videoIntelligence": 0.05,
      "speechToText": 0.02,
      "total": 0.07
    }
  }
}
```

### 3.10 PROGRESS TRACKING: Real-Time Updates
**Step 30**: Throughout processing, update progress in real-time:
- After each segment completes: Update `segmentsProcessed` counter
- Calculate percentage: `(segmentsProcessed / totalSegments) * 100`
- Update Firestore: `videos/{videoId}.analysisProgress = percentage`
- Frontend subscribes to this field and updates progress bar in real-time

### 3.11 COMPLETION: Finalize Analysis
**Step 31**: When all segments are processed:
- Update video status: `status: "analysis_complete"`
- Set completion timestamp: `analyzedAt: currentTimestamp`
- Calculate total costs and processing time
- Update segment count: `segmentCount: actualSegmentCount`
- Clean up temporary processing files in container

**Step 32**: Generate video-level summary:
- Total people detected across all segments
- Most common objects/activities
- Primary scene locations
- Total speaking time per person
- Overall video themes and context

**Step 33**: Update UI in real-time:
- Video library card changes from "Analyzing 87%" to "‚úì Ready to Search"
- User can now search and browse this video's segments
- Segments are available for timeline viewing and expansion

### 3.12 ERROR HANDLING: Recovery Process
**Step 34**: If any step fails:
- Save current progress state to Firestore
- Mark specific failed segments for retry
- Continue processing remaining segments
- Retry failed segments up to 3 times with exponential backoff

**Step 35**: If processing completely fails:
- Update status: `status: "analysis_failed"`
- Save error details and partial results
- Notify user with retry option
- Preserve uploaded video for manual reprocessing

### 3.13 COST OPTIMIZATION: Smart Processing
**Step 36**: Implement cost controls:
- Skip segments that are completely black/silent (scene transitions)
- Batch similar segments for bulk API processing
- Use lower-cost APIs for simple segments (static scenes)
- Stop processing if user cost quota is exceeded

### 3.14 USER EXPERIENCE: Timeline Interface
**Step 37**: After analysis completion, user sees:
- Timeline with 5-second segment thumbnails
- Click segment ‚Üí View detailed analysis
- Drag to select multiple segments ‚Üí Expand to longer clip
- Search functionality across all segments
- Real-time segment browsing with rich metadata display

## Processing Time & Cost Estimates
**10-minute video (120 segments)**:
- Segmentation: 2 minutes
- Audio Analysis: 3 minutes (parallel processing)
- Visual Analysis: 8 minutes (parallel batches)
- Cross-modal linking: 1 minute
- Description generation: 2 minutes
- Data storage: 1 minute
- **Total Processing Time**: ~17 minutes
- **API Costs**: ~$15-20 per 10-minute video

**Key Benefits of 5-Second Segments**:
- **Granular Control**: Users can expand/contract clips precisely
- **Fast Search**: Quick preview of exact moments
- **Efficient Processing**: Parallel segment analysis
- **Cost Effective**: Skip boring/empty segments
- **User Experience**: Immediate visual timeline browsing

---

## üìã Phase 4: Search & Compilation (Future)

### User Search Flow
1. **Natural Language Query**: User types "exciting goal moments"
2. **Semantic Search**: Convert query to vector, find matching scenes
3. **Director Algorithm**: Select best scenes within time constraints  
4. **Video Compilation**: Extract and stitch scenes using FFmpeg
5. **Delivery**: Present compiled video to user

---

## üîÑ Cross-Phase Considerations

### Data Flow Principles
- **User-Centric**: All data tied to authenticated user accounts
- **Resumable**: Every long-running process can be interrupted and resumed
- **Real-time**: UI updates immediately reflect backend state changes
- **Secure**: All operations require authentication and authorization
- **Cost-Efficient**: Direct storage uploads, serverless compute
- **Scalable**: Architecture handles 1 user or 10,000 users identically

### Error Handling Strategy
- **Graceful Degradation**: System works with partial failures
- **User Communication**: Clear error messages and recovery options
- **Automatic Retry**: Exponential backoff for transient failures
- **Manual Recovery**: Admin tools for stuck processes
- **Monitoring**: Real-time alerts for system issues

### Security Considerations
- **Authentication**: Firebase Auth with social login options
- **Authorization**: User can only access their own content
- **Data Privacy**: Upload sessions and videos tied to user accounts
- **Content Validation**: File type and size restrictions enforced
- **Rate Limiting**: Prevent abuse with quota systems

---

## üìã Phase 5: Manual Video Editing Interface

### üéØ Phase 5 Overview
Create an intuitive video editing interface where users can manually select and arrange clips from their analyzed videos to create custom compilations.

**Core Philosophy**: AI Analysis ‚Üí Manual Selection ‚Üí Timeline Editing ‚Üí Video Export

**Key User Flow**: 
1. Select video from library ‚Üí Edit page
2. Search for scenes within that video ‚Üí Add clips to timeline
3. Arrange, trim, and adjust clips ‚Üí Export final video

---

## üìã 5.1 NAVIGATION & PAGE STRUCTURE

### 5.1.1 App Navigation Enhancement
**Updated Navbar Structure**:
- **Upload Page** (`/upload`) - Primary entry point (existing)
- **Projects Page** (`/projects`) - Project management (new)
- **Edit Page** (`/edit/:videoId`) - Video editing interface (new)

**Navigation Rules**:
- Upload and Projects pages have navbar to switch between them
- Edit page is a separate focused interface (minimal navbar)
- Projects page only appears in navbar AFTER user creates their first project

### 5.1.2 Page Hierarchy & User Flow
```
/upload (main entry)
    ‚Üì (user uploads & analyzes video)
/upload (library shows analyzed videos)
    ‚Üì (user clicks video card)
/edit/:videoId (editing interface)
    ‚Üì (user adds first clip)
/projects (projects page appears in navbar)
    ‚Üì (user can return to continue editing)
/edit/:projectId (resume editing existing project)
```

---

## üìã 5.2 PROJECTS PAGE DESIGN

### 5.2.1 Projects List Interface
**Projects Page Layout** (`/projects`):
```jsx
<ProjectsPage>
  <PageHeader>
    <Title>Your Video Projects</Title>
    <CreateButton>Start New Project</CreateButton>
  </PageHeader>
  
  <ProjectsGrid>
    {projects.map(project => (
      <ProjectCard key={project.id}>
        <Thumbnail src={project.thumbnailUrl} />
        <ProjectInfo>
          <Title>{project.name}</Title>
          <SourceVideo>{project.sourceVideoName}</SourceVideo>
          <Stats>
            {project.clipCount} clips ‚Ä¢ {project.totalDuration}
          </Stats>
          <LastEdited>{project.lastEditedAt}</LastEdited>
        </ProjectInfo>
        <Actions>
          <EditButton />
          <DeleteButton />
          <ExportButton />
        </Actions>
      </ProjectCard>
    ))}
  </ProjectsGrid>
</ProjectsPage>
```

### 5.2.2 Project Creation Flow
**New Project Initiation**:
1. **From Upload Page**: User clicks video ‚Üí Goes to `/edit/:videoId` (creates project on first clip)
2. **From Projects Page**: User clicks "Start New Project" ‚Üí Modal to select source video ‚Üí Goes to edit page
3. **Auto-naming**: Projects auto-named as "{VideoName} - Edit 1" (user can rename)

### 5.2.3 Project Management Features
**Project Operations**:
- **Edit**: Continue editing ‚Üí `/edit/:projectId`
- **Duplicate**: Create copy of project with same clips
- **Export**: Generate final video file
- **Delete**: Remove project (confirm dialog)
- **Rename**: Inline editing of project name

---

## üìã 5.3 EDIT PAGE ARCHITECTURE

### 5.3.1 Edit Page Layout & States
**Edit Page URL Structure**:
- **New Project**: `/edit/:videoId` (creates project when first clip added)
- **Existing Project**: `/edit/:projectId` (resumes editing)

**Page Layout**:
```jsx
<EditPage>
  <Header>
    <BackButton />
    <ProjectTitle editable />
    <SaveStatus />
    <ExportButton />
  </Header>
  
  <MainContent>
    {timeline.clips.length === 0 ? (
      <EmptyState>
        <SearchInput placeholder="Describe the scene you want to find..." />
      </EmptyState>
    ) : (
      <>
        <VideoPreview>
          <Player src={compiledPreviewUrl} />
          <PlaybackControls />
        </VideoPreview>
        <AddClipButton onClick={openSearchModal} />
      </>
    )}
  </MainContent>
  
  <Timeline>
    {clips.map(clip => (
      <ClipItem key={clip.id}>
        <Thumbnail />
        <ClipControls>
          <TrimStart />
          <Duration />
          <TrimEnd />
        </ClipControls>
        <RemoveButton />
      </ClipItem>
    ))}
    <TotalDuration />
  </Timeline>
</EditPage>
```

### 5.3.2 State Management Strategy
**Edit Page States**:
1. **Empty Project**: Large search input prominently displayed
2. **Building Project**: Video preview + timeline with clips + search button
3. **Saving**: Auto-save progress indicators
4. **Exporting**: Progress bar for video compilation

---

## üìã 5.4 CLIP SEARCH & SELECTION SYSTEM

### 5.4.1 Scene Search Interface
**Search Modal Design**:
```jsx
<ClipSearchModal>
  <SearchHeader>
    <SearchInput 
      placeholder="Describe what you're looking for..."
      autoFocus
    />
    <VideoInfo>
      Searching in: {currentVideo.name}
    </VideoInfo>
  </SearchHeader>
  
  <SearchResults>
    {results.map(segment => (
      <SegmentOption key={segment.id}>
        <Thumbnail src={segment.thumbnailUrl} />
        <SegmentInfo>
          <TimeRange>{segment.startTime} - {segment.endTime}</TimeRange>
          <Description>{segment.searchableText}</Description>
          <ConfidenceScore>{segment.relevanceScore}% match</ConfidenceScore>
        </SegmentInfo>
        <AddButton onClick={() => addClipToTimeline(segment)} />
      </SegmentOption>
    ))}
  </SearchResults>
  
  <ModalActions>
    <CancelButton />
    <SearchTips />
  </ModalActions>
</ClipSearchModal>
```

### 5.4.2 Search Implementation Logic
**Search Process**:
1. **User Types Query**: "cooking scene" or "people laughing"
2. **Real-time Search**: Query segments from current video only
3. **Semantic Matching**: Use existing `searchableText` field from Phase 3
4. **Results Display**: Show all matching segments sorted by relevance
5. **Quick Add**: Single click to add segment to timeline

**Search Scope**:
- **Video-Specific**: Only search within the currently selected video
- **All Segments**: Search across all analyzed segments from that video
- **Real-time**: Results update as user types (debounced)

### 5.4.3 Clip Addition Workflow
**Adding Clips to Timeline**:
```javascript
// User clicks segment in search results
function addClipToTimeline(segment) {
  const newClip = {
    id: generateClipId(),
    segmentId: segment.id,
    videoId: segment.videoId,
    originalStart: segment.startTime,
    originalEnd: segment.endTime,
    trimmedStart: segment.startTime,    // User can adjust
    trimmedEnd: segment.endTime,        // User can adjust
    position: timeline.clips.length,    // Order in timeline
    thumbnailUrl: segment.thumbnailUrl,
    addedAt: new Date()
  };
  
  // Add to timeline
  timeline.clips.push(newClip);
  
  // Auto-save project
  saveProjectToFirestore();
  
  // Close search modal
  closeSearchModal();
  
  // Update preview video
  generatePreview();
}
```

---

## üìã 5.5 TIMELINE EDITING INTERFACE

### 5.5.1 Timeline Component Design
**Timeline Layout**:
```jsx
<Timeline>
  <TimelineHeader>
    <TotalDuration>{calculateTotalDuration()}</TotalDuration>
    <TimelineControls>
      <ZoomIn />
      <ZoomOut />
      <FitToWidth />
    </TimelineControls>
  </TimelineHeader>
  
  <ClipTrack>
    {clips.map((clip, index) => (
      <ClipItem 
        key={clip.id}
        clip={clip}
        position={index}
        onDrag={handleClipReorder}
        onTrim={handleClipTrim}
        onRemove={handleClipRemove}
      />
    ))}
    <AddClipButton />
  </ClipTrack>
  
  <TimelineRuler>
    {/* Time markers: 0:00, 0:05, 0:10, etc. */}
  </TimelineRuler>
</Timeline>
```

### 5.5.2 Clip Manipulation Features
**Individual Clip Controls**:
```jsx
<ClipItem>
  <ClipThumbnail src={clip.thumbnailUrl} />
  
  <ClipControls>
    <TrimButton 
      side="left" 
      onClick={() => adjustClipStart(clip.id, -1)} 
      disabled={clip.trimmedStart <= clip.originalStart}
    />
    
    <ClipDuration>
      {formatDuration(clip.trimmedEnd - clip.trimmedStart)}
    </ClipDuration>
    
    <TrimButton 
      side="right" 
      onClick={() => adjustClipEnd(clip.id, +1)}
      disabled={clip.trimmedEnd >= clip.originalEnd}
    />
  </ClipControls>
  
  <ClipActions>
    <DragHandle />
    <RemoveButton />
  </ClipActions>
</ClipItem>
```

**Clip Adjustment Logic**:
```javascript
// Extend/trim clip duration
function adjustClipStart(clipId, seconds) {
  const clip = findClipById(clipId);
  const newStart = clip.trimmedStart + seconds;
  
  // Validate bounds
  if (newStart >= clip.originalStart && newStart < clip.trimmedEnd) {
    clip.trimmedStart = newStart;
    saveProject();
    regeneratePreview();
  }
}

function adjustClipEnd(clipId, seconds) {
  const clip = findClipById(clipId);
  const newEnd = clip.trimmedEnd + seconds;
  
  // Validate bounds  
  if (newEnd <= clip.originalEnd && newEnd > clip.trimmedStart) {
    clip.trimmedEnd = newEnd;
    saveProject();
    regeneratePreview();
  }
}
```

### 5.5.3 Timeline Interactions
**User Interactions**:
- **Drag & Drop**: Reorder clips by dragging
- **Trim Controls**: +/- buttons to extend/contract clips
- **Click to Play**: Click clip thumbnail to preview that segment
- **Remove**: X button to delete clip from timeline
- **Add More**: + button to open search modal for more clips

---

## üìã 5.6 VIDEO PREVIEW SYSTEM

### 5.6.1 Preview Generation
**Real-time Preview Strategy**:
- **Client-side Preview**: Concatenate video URLs for quick preview
- **Server-side Compilation**: Generate actual compiled video for export
- **Thumbnail Timeline**: Show visual timeline of selected clips

**Preview Implementation**:
```javascript
// Generate client-side preview URL
function generatePreviewUrl(clips) {
  // Create playlist of video segments with time ranges
  const playlist = clips.map(clip => ({
    url: clip.segmentUrl,
    startTime: clip.trimmedStart - clip.originalStart,
    endTime: clip.trimmedEnd - clip.originalStart
  }));
  
  // Use video.js or similar to play sequential clips
  return createVideoPlaylist(playlist);
}
```

### 5.6.2 Preview Player Features
**Video Player Controls**:
- **Play/Pause**: Standard video controls
- **Scrub Timeline**: Seek through entire compilation
- **Clip Boundaries**: Visual markers showing clip transitions
- **Current Time**: Show position within compilation
- **Full Screen**: Expand preview for better viewing

---

## üìã 5.7 DATABASE SCHEMA

### 5.7.1 Projects Collection Structure
**Projects Collection** (`projects/{projectId}`):
```json
{
  "id": "proj_abc123",
  "userId": "user123",
  "name": "Family Dinner Highlights",
  "sourceVideoId": "vid789",
  "sourceVideoName": "Family Dinner 2024",
  
  "clips": [
    {
      "id": "clip_001",
      "segmentId": "vid789_segment_023",
      "position": 0,
      "originalStart": 110.0,
      "originalEnd": 115.0,
      "trimmedStart": 111.0,
      "trimmedEnd": 114.5,
      "thumbnailUrl": "segments/vid789_segment_023.jpg",
      "addedAt": "2024-01-15T14:22:30Z"
    },
    {
      "id": "clip_002", 
      "segmentId": "vid789_segment_067",
      "position": 1,
      "originalStart": 330.0,
      "originalEnd": 335.0,
      "trimmedStart": 330.0,
      "trimmedEnd": 333.0,
      "thumbnailUrl": "segments/vid789_segment_067.jpg", 
      "addedAt": "2024-01-15T14:23:15Z"
    }
  ],
  
  "metadata": {
    "totalDuration": 6.5,
    "clipCount": 2,
    "createdAt": "2024-01-15T14:22:30Z",
    "lastEditedAt": "2024-01-15T14:25:45Z",
    "version": 1
  },
  
  "export": {
    "status": "none", // none, processing, completed, failed
    "exportedAt": null,
    "downloadUrl": null,
    "settings": {
      "quality": "1080p",
      "format": "mp4"
    }
  }
}
```

### 5.7.2 User Collection Extensions
**Add to Users Collection** (`users/{userId}`):
```json
{
  // ... existing user fields ...
  "projects": {
    "totalProjects": 3,
    "lastProjectId": "proj_abc123", 
    "hasCreatedProject": true
  }
}
```

### 5.7.3 Database Queries & Indexes
**Required Firestore Indexes**:
- `projects`: `userId` + `lastEditedAt` (descending)
- `projects`: `userId` + `sourceVideoId`
- `projects`: `userId` + `export.status`

**Common Queries**:
```javascript
// Get user's projects (for projects page)
const userProjects = await firebase
  .collection('projects')
  .where('userId', '==', currentUser.uid)
  .orderBy('lastEditedAt', 'desc')
  .get();

// Get projects for specific video
const videoProjects = await firebase
  .collection('projects')
  .where('userId', '==', currentUser.uid)
  .where('sourceVideoId', '==', videoId)
  .get();
```

---

## üìã 5.8 SAVE & EXPORT SYSTEM

### 5.8.1 Auto-Save Strategy
**Real-time Saving**:
- **Auto-save Triggers**: Add clip, remove clip, trim clip, reorder clips
- **Debounced Saves**: Wait 2 seconds after last edit before saving
- **Save Status Indicator**: "Saving...", "Saved", "Error - Retry"
- **Conflict Resolution**: Handle multiple device editing (last write wins)

**Save Implementation**:
```javascript
// Debounced save function
const debouncedSave = debounce(async (projectData) => {
  try {
    setSaveStatus('saving');
    await updateProject(projectData);
    setSaveStatus('saved');
  } catch (error) {
    setSaveStatus('error');
    console.error('Save failed:', error);
  }
}, 2000);

// Trigger save on any timeline change
function onTimelineChange() {
  const projectData = {
    clips: timeline.clips,
    metadata: {
      totalDuration: calculateTotalDuration(),
      clipCount: timeline.clips.length,
      lastEditedAt: new Date()
    }
  };
  
  debouncedSave(projectData);
}
```

### 5.8.2 Video Export Pipeline
**Export Process**:
1. **User Clicks Export**: Validates project has clips
2. **Queue Export Job**: Create Cloud Task for video compilation
3. **Server Processing**: Download segments, trim, concatenate with FFmpeg
4. **Upload Result**: Save compiled video to Cloud Storage
5. **Notify User**: Update project with download URL

**Export Job Structure**:
```javascript
const exportJob = {
  projectId: "proj_abc123",
  userId: "user123",
  clips: project.clips,
  settings: {
    quality: "1080p",
    format: "mp4",
    fadeTransitions: true
  },
  status: "queued"
};
```

---

## üìã 5.9 USER EXPERIENCE FLOWS

### 5.9.1 First-Time User Journey
**Complete First Project Flow**:
1. **Upload Video**: User uploads and analyzes their first video
2. **Library View**: Video appears in library with "Ready" status  
3. **Start Editing**: User clicks video card ‚Üí Goes to `/edit/:videoId`
4. **Empty Timeline**: Large search input: "Describe the scene you want..."
5. **First Search**: User types "people talking" ‚Üí Modal shows matching segments
6. **Add First Clip**: User clicks segment ‚Üí Clip appears in timeline
7. **Project Created**: System creates project, navbar now shows "Projects"
8. **Continue Editing**: User adds more clips, trims, reorders
9. **Export Video**: User exports final compilation

### 5.9.2 Returning User Experience
**Experienced User Flow**:
1. **Projects Page**: User visits `/projects` to see existing projects
2. **Resume Editing**: Click project ‚Üí Goes to `/edit/:projectId`
3. **Timeline Loaded**: Project loads with existing clips in timeline
4. **Quick Edits**: User adds clips via search button, adjusts timing
5. **Export & Share**: Generate final video for sharing

### 5.9.3 Edge Cases & Error Handling
**Error Scenarios**:
- **Deleted Source Video**: Show error, offer to remove project
- **Corrupted Project**: Backup recovery, partial restoration
- **Export Failures**: Retry mechanism, quality fallback options
- **Concurrent Editing**: Last-save-wins with conflict notifications

---

## üìã 5.10 TECHNICAL IMPLEMENTATION PRIORITIES

### 5.10.1 Development Phases
**Phase 5.A - Basic Structure**:
1. Create projects page and navigation
2. Build edit page layout and routing
3. Implement basic timeline component
4. Set up projects database schema

**Phase 5.B - Core Editing**:
1. Build clip search modal and functionality
2. Implement timeline clip management (add, remove, reorder)
3. Add clip trimming controls (+/- buttons)
4. Create auto-save system

**Phase 5.C - Preview & Export**:
1. Implement video preview generation
2. Build export pipeline with FFmpeg
3. Add progress tracking and notifications
4. Polish UI and error handling

### 5.10.2 Component Architecture
**Key React Components**:
- `ProjectsPage` - Projects list and management
- `EditPage` - Main editing interface
- `Timeline` - Clip timeline and controls
- `ClipItem` - Individual clip with trim controls
- `ClipSearchModal` - Search and add clips
- `VideoPreview` - Preview player
- `ExportModal` - Export settings and progress

---

*Phase 5 provides users with an intuitive, manual video editing interface that leverages the AI analysis from Phase 3 to help users quickly find and arrange the exact moments they want in their final videos.*

---

*This map provides the logical flow and decision points for each phase. Detailed implementation code is maintained in the todo file.*
